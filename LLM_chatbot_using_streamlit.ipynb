{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csipbasWGoml"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2ZaukEX2dpEdTGQatDve7dPpmGL_3u8Eur4MtR4aXCYparH96"
      ],
      "metadata": {
        "id": "sGasBijqG5Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok"
      ],
      "metadata": {
        "id": "SuiMvmIeG-sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu\n",
        "!pip install PyPDF2\n",
        "!pip install langchain sentence_transformers\n",
        "!pip install huggingface_hub\n",
        "!pip install unstructured"
      ],
      "metadata": {
        "id": "5M07GDg8iq7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import langchain\n",
        "import PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "import os\n",
        "from langchain.llms import HuggingFaceHub, OpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "import random\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'\n",
        "\n",
        "GREET_INPUTS = ['hello', 'hi', 'hey', 'yo', 'whats up', \"what's up\"]\n",
        "GREET_RESPONSES = ['hi', 'hey', \"what's up\", 'I am glad! You are talking to me']\n",
        "\n",
        "\n",
        "def get_text_docs(text):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator='\\n',\n",
        "        chunk_size=100,\n",
        "        chunk_overlap=20,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_vectorestore(text_chunks):\n",
        "    embeddings = HuggingFaceEmbeddings()\n",
        "    vectorestore = FAISS.from_documents(documents=text_chunks, embedding=embeddings)\n",
        "    if bool(vectorestore):\n",
        "      st.write(\"You can Start Chatting\")\n",
        "      #st.write(vectorestore)\n",
        "    return vectorestore\n",
        "\n",
        "\n",
        "def greet(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREET_INPUTS:\n",
        "            return random.choice(GREET_RESPONSES)\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "#Creating chat interface\n",
        "    favicon_url = 'https://th.bing.com/th/id/OIP.4RnNLKNxFID9JiAfMeWf8gHaHa?rs=1&pid=ImgDetMain'\n",
        "    st.set_page_config(page_title='LLM Integrated ChatBot', page_icon=favicon_url)\n",
        "\n",
        "    st.header('Your Friendly Chatbot')\n",
        "    st.write('I am a conversational bot, ready to answer your query! To end the chat type \"exit\".')\n",
        "\n",
        "    data = None\n",
        "\n",
        "    if 'vectorstore' not in st.session_state:\n",
        "      st.session_state.vectorstore = None\n",
        "\n",
        "    with st.sidebar:\n",
        "      st.subheader('Past Yor URL Links')\n",
        "      url_1 = st.sidebar.text_input('Enter your first URL here')\n",
        "      url_2 = st.sidebar.text_input('Enter your second URL here')\n",
        "      url_3 = st.sidebar.text_input('Enter your third URL here')\n",
        "\n",
        "      urls = [url_1, url_2, url_3]\n",
        "\n",
        "      if st.button('Process'):\n",
        "        loaders = UnstructuredURLLoader(urls=urls)\n",
        "        data = loaders.load()\n",
        "\n",
        "    if data is not None:\n",
        "      docs = get_text_docs(data)\n",
        "      st.session_state.vectorstore = get_vectorestore(docs)\n",
        "\n",
        "\n",
        "\n",
        "    if 'message' not in st.session_state:\n",
        "        st.session_state.message = []\n",
        "\n",
        "\n",
        "    for message in st.session_state.message:\n",
        "        with st.chat_message(message['role']):\n",
        "            st.markdown(message['content'])\n",
        "\n",
        "    #USER RESPONSE\n",
        "    user_question = st.chat_input('Enter your query here')\n",
        "\n",
        "    if user_question:\n",
        "        with st.chat_message('user'):\n",
        "            st.markdown(user_question)\n",
        "        st.session_state.message.append({'role': \"user\", \"content\": user_question})\n",
        "\n",
        "\n",
        "    if user_question:\n",
        "\n",
        "        response = ''\n",
        "        if(user_question!='exit'):\n",
        "            if(user_question == 'thanks' or user_question == 'thank you'):\n",
        "                with st.chat_message('assistant'):\n",
        "                    response = response + \"You are Welcome\"\n",
        "                    st.markdown(response)\n",
        "                st.session_state.message.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "            else:\n",
        "                if(greet(user_question)!=None):\n",
        "                    with st.chat_message('assistant'):\n",
        "                        response = response + greet(user_question)\n",
        "                        st.markdown(response)\n",
        "                    st.session_state.message.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "                else:\n",
        "                    repo_id = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "                    llm = HuggingFaceHub(\n",
        "                        repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_length\": 500})\n",
        "\n",
        "                    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "                    chain = ConversationalRetrievalChain.from_llm(\n",
        "                        llm=llm,\n",
        "                        memory=memory,\n",
        "                        retriever=st.session_state.vectorstore.as_retriever())\n",
        "\n",
        "                    #st.write(\"chain created\")\n",
        "\n",
        "                    with st.chat_message('assistant'):\n",
        "                        chain_answer = chain({'question':user_question, \"chat_history\": []}, return_only_outputs=True)\n",
        "                        response = response + chain_answer['answer']\n",
        "                        st.markdown(response)\n",
        "\n",
        "                    st.session_state.message.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "\n",
        "        else:\n",
        "            with st.chat_message('assistant'):\n",
        "                response = response + 'GoodBye!'\n",
        "                st.markdown(response)\n",
        "            st.session_state.message.append({\"role\": \"assistant\", \"content\": response})\n",
        "            st.session_state.message = []\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXZmCvC_HBN7",
        "outputId": "8d8d4a65-4c52-4ca2-c7fb-d0f73e92298c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "metadata": {
        "id": "tNkRvYr946WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pgrep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipQizcDYh7wg",
        "outputId": "52e909e6-c2bf-4f2a-86fc-f4580bb8f01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "# Setup a tunnel to the streamlit port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "public_url"
      ],
      "metadata": {
        "id": "ymG7m0joh9FB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d9710c6-9f1f-4a04-d6b8-a9b9c4679fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://5e2f-35-233-143-185.ngrok-free.app\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-xQEnv2iMk8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}